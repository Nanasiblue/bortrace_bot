# -*- coding: utf-8 -*-
"""main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VkynHuCEGFtfFkLZw4hDUfArvGWD6Y5G
"""

import os
import pandas as pd
import numpy as np
import pickle
import re
import requests
import concurrent.futures
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime, timedelta, timezone

# ==========================================
# è¨­å®š: Discord Webhook (GitHub Secretsã‹ã‚‰èª­ã¿è¾¼ã¿)
# ==========================================
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")

# æ—¥æœ¬æ™‚é–“ã®å®šç¾© (GitHubã‚µãƒ¼ãƒãƒ¼ã¯UTCãªã®ã§å¿…é ˆ)
JST = timezone(timedelta(hours=9), 'JST')

# ==========================================
# 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ (ã‚¯ãƒ©ã‚¹å®šç¾©)
# ==========================================
class BoatRaceScraperV4:
    BASE_URL = "https://www.boatrace.jp/owpc/pc/race/beforeinfo"
    LIST_URL = "https://www.boatrace.jp/owpc/pc/race/racelist"
    INDEX_URL = "https://www.boatrace.jp/owpc/pc/race/index"

    COURSE_MAP = {
        "æ¡ç”Ÿ": "01", "æˆ¸ç”°": "02", "æ±Ÿæˆ¸å·": "03", "å¹³å’Œå³¶": "04", "å¤šæ‘©å·": "05",
        "æµœåæ¹–": "06", "è’²éƒ¡": "07", "å¸¸æ»‘": "08", "æ´¥": "09", "ä¸‰å›½": "10",
        "ã³ã‚ã“": "11", "ä½ä¹‹æ±Ÿ": "12", "å°¼å´": "13", "é³´é–€": "14", "ä¸¸äº€": "15",
        "å…å³¶": "16", "å®®å³¶": "17", "å¾³å±±": "18", "ä¸‹é–¢": "19", "è‹¥æ¾": "20",
        "èŠ¦å±‹": "21", "ç¦å²¡": "22", "å”æ´¥": "23", "å¤§æ‘": "24"
    }

    def __init__(self):
        self.headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    def _get_soup(self, url, retries=2):
        for i in range(retries):
            try:
                res = requests.get(url, headers=self.headers, timeout=15)
                res.raise_for_status()
                return BeautifulSoup(res.content, "html.parser")
            except:
                continue
        return None

    def fetch_active_courses(self, date_str):
        soup = self._get_soup(f"{self.INDEX_URL}?hd={date_str}")
        if not soup: return []
        active_courses = []
        inv_map = {v: k for k, v in self.COURSE_MAP.items()}
        for link in soup.select("a[href*='jcd=']"):
            m = re.search(r"jcd=(\d{2})", link['href'])
            if m and m.group(1) in inv_map:
                active_courses.append(inv_map[m.group(1)])
        return sorted(list(set(active_courses)))

    def fetch_race_data(self, course, rno, date_str):
        jcd = self.COURSE_MAP[course]
        try:
            # 1. å‡ºèµ°è¡¨ã‹ã‚‰åŸºæœ¬æƒ…å ±ã¨ç· åˆ‡æ™‚åˆ»ã‚’å–å¾—
            soup_list = self._get_soup(f"{self.LIST_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_list: return None

            deadline_str = "00:00"
            text_full = soup_list.get_text()
            match_time = re.search(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", text_full)
            if match_time: deadline_str = match_time.group(1).zfill(5)

            bodies = soup_list.select("tbody.is-fs12")
            if not bodies: bodies = soup_list.select("tbody")

            row1 = None
            for b_idx, b in enumerate(bodies):
                if "ï¼‘" in b.text[:10]: row1 = b; break
            if not row1: return None

            rank_1, win_rate_1 = "B2", 0.0
            rank_match = re.search(r"/ ([AB][12])", row1.text)
            if rank_match: rank_1 = rank_match.group(1)

            td_texts = [td.text.strip().replace("\n", " ") for td in row1.find_all("td")]
            all_rates = re.findall(r"(\d\.\d{2})", " ".join(td_texts))
            if all_rates: win_rate_1 = float(all_rates[0])

            # 2. ç›´å‰æƒ…å ±ã‹ã‚‰å±•ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            soup_info = self._get_soup(f"{self.BASE_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_info or "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in soup_info.text: return None

            weather = soup_info.select_one(".weather1")
            wind_speed, wave = 0, 0
            if weather:
                txt = weather.text
                w_m = re.search(r"é¢¨é€Ÿ.*?(\d+)m", txt)
                h_m = re.search(r"æ³¢é«˜.*?(\d+)cm", txt)
                if w_m: wind_speed = int(w_m.group(1))
                if h_m: wave = int(h_m.group(1))

            table = soup_info.select_one(".is-w748")
            if not table: return None
            rows = table.select("tbody")
            ex_times, st_list = [], []
            for i in range(6):
                tds = rows[i].select("td")
                if len(tds) < 5 or not tds[4].text.strip(): return None
                ex_times.append(float(tds[4].text.strip()))
                st_text = tds[2].select_one(".is-fs11").text.strip() if tds[2].select_one(".is-fs11") else "0.00"
                st_list.append(float(re.search(r"(\.\d+)", st_text).group(1)) if re.search(r"(\.\d+)", st_text) else 0.0)

            ex_rank = pd.Series(ex_times).rank(method="min").tolist()

            data = {
                "wind_speed": wind_speed, "wave": wave,
                "ex_rank_1": ex_rank[0], "rank_1": rank_1, "win_rate_1": win_rate_1,
                "deadline": deadline_str
            }
            for i in range(6):
                data[f"st_{i+1}"] = st_list[i]
                data[f"ex_time_{i+1}"] = ex_times[i]
            return data
        except: return None

# ==========================================
# 2. äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
def predict_single(model, config, scraper, course, rno, date_str):
    try:
        race_data = scraper.fetch_race_data(course, rno, date_str)
        if not race_data: return None, -1 # ãƒ‡ãƒ¼ã‚¿ãªã—

        # ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        rank_map = {"A1": 4, "A2": 3, "B1": 2, "B2": 1}
        rank_val_1 = rank_map.get(race_data["rank_1"], 2)
        is_debuff_1 = 1 if (rank_val_1 <= 2 and race_data["ex_rank_1"] >= 4) else 0

        input_data = race_data.copy()
        input_data["rank_val_1"] = rank_val_1
        input_data["is_debuff_1"] = is_debuff_1

        # äºˆæ¸¬å®Ÿè¡Œ
        input_df = pd.DataFrame([input_data])[config["features"]]
        prob = model.predict(input_df)[0]

        # è²·ã„ç›®ç”Ÿæˆ
        ex_times_26 = {i: race_data[f"ex_time_{i}"] for i in range(2, 7)}
        axis_boat = min(ex_times_26, key=ex_times_26.get)

        res_dict = {
            "å ´å": course, "ãƒ¬ãƒ¼ã‚¹": f"{rno}R", "ç· åˆ‡": race_data['deadline'],
            "ç¢ºç‡": f"{prob:.1%}", "è²·ã„ç›®": f"{axis_boat}é ­æµ", "rank1": race_data["rank_1"]
        }

        if prob >= config["best_threshold"]:
            return res_dict, 1 # æ¨å¥¨
        return res_dict, 0 # æ¨å¥¨å¤–

    except Exception:
        return None, -2

# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ (å®‰å…¨ç‰ˆãƒ‘ãƒˆãƒ­ãƒ¼ãƒ«)
# ==========================================
def run_github_patrol():
    print("ğŸ‘® Smart Patrol Starting (JST)...")
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model_path = Path("boatrace_model_v3.pkl")
    config_path = Path("model_config.pkl")
    
    if not model_path.exists():
        print("Error: Model files not found.")
        return

    with open(model_path, "rb") as f: model = pickle.load(f)
    with open(config_path, "rb") as f: config = pickle.load(f)

    scraper = BoatRaceScraperV4()
    
    # ç¾åœ¨æ™‚åˆ»(JST)
    now_jst = datetime.now(JST)
    date_str = now_jst.strftime("%Y%m%d")
    
    print(f"Current Time (JST): {now_jst.strftime('%H:%M')}")

    # é–‹å‚¬å ´å–å¾—
    courses = scraper.fetch_active_courses(date_str)
    if not courses:
        print("No races today.")
        return

    print(f"Active Courses: {len(courses)} venues")
    hits = []

    # === ğŸ›¡ï¸ ã“ã“ã‚’å®‰å…¨ä»•æ§˜ã«å¤‰æ›´ ===
    # ä¸¦åˆ—å‡¦ç†(ThreadPool)ã‚’ã‚„ã‚ã¦ã€1ã¤ãšã¤é †ç•ªã«å‡¦ç†ã™ã‚‹
    # ã“ã‚Œãªã‚‰ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„
    
    for c in courses:
        for r in range(1, 13):
            try:
                # 1ç§’ä¼‘ã‚€ 
                time.sleep(1) 
                
                # äºˆæ¸¬å®Ÿè¡Œ
                res, status = predict_single(model, config, scraper, c, r, date_str)
                
                # ã€Œæ¨å¥¨(status=1)ã€ã‹ã¤ã€Œãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸã€ã®å ´åˆ
                if status == 1 and res:
                    race_dt_str = f"{date_str} {res['ç· åˆ‡']}"
                    race_dt = datetime.strptime(race_dt_str, "%Y%m%d %H:%M").replace(tzinfo=JST)
                    
                    diff = race_dt - now_jst
                    minutes_left = diff.total_seconds() / 60
                    
                    # ç· åˆ‡ã¾ã§10åˆ†ã€œ35åˆ†
                    if 10 <= minutes_left <= 35:
                        print(f"Found! {c} {r}R") # ãƒ­ã‚°ç¢ºèªç”¨
                        hits.append(res)
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚æ­¢ã¾ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹
                # print(f"Error at {c} {r}R: {e}") 
                pass

    # é€šçŸ¥å‡¦ç†
    if hits:
        print(f"ğŸ¯ Found {len(hits)} actionable races!")
        hits.sort(key=lambda x: x['ç· åˆ‡'])
        
        content = "ğŸ¤– **æ¿€ã‚¢ãƒ„ç›´å‰é€šçŸ¥** (ç· åˆ‡10ã€œ30åˆ†å‰)\n"
        for r in hits:
            content += f"**{r['å ´å']} {r['ãƒ¬ãƒ¼ã‚¹']}** (ç· åˆ‡ {r['ç· åˆ‡']})\n"
            content += f"ç¢ºç‡: `{r['ç¢ºç‡']}`  è²·ã„ç›®: `{r['è²·ã„ç›®']}`\n"
            content += "------------------------\n"
        
        if DISCORD_WEBHOOK_URL:
            requests.post(DISCORD_WEBHOOK_URL, json={"content": content})
            print("Notification sent to Discord.")
        else:
            print("Webhook URL is missing.")
    else:
        print("No races found in the target time window.")

if __name__ == "__main__":
    run_github_patrol()
