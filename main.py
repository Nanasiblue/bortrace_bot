import os
import pandas as pd
import numpy as np
import pickle
import re
import requests
import time
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime, timedelta, timezone

# ==========================================
# è¨­å®š
# ==========================================
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
JST = timezone(timedelta(hours=9), 'JST')

# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ã“ã“ã«å…¥ã‚Œã¦ãã ã•ã„ï¼‰
MODEL_PATH = Path("final_model_v4.pkl")
CONFIG_PATH = Path("model_config_v4.pkl")
# é€šçŸ¥æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ï¼‰
LOG_FILE = Path("notified_races.log")

# ==========================================
# å…±é€šãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
def is_already_notified(race_id):
    if not LOG_FILE.exists(): return False
    with open(LOG_FILE, "r") as f:
        notified_races = f.read().splitlines()
    return race_id in notified_races

def save_notified_race(race_id):
    with open(LOG_FILE, "a") as f:
        f.write(race_id + "\n")

# ==========================================
# 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ (v5: é«˜ç²¾åº¦ãƒ»å…¨è‰‡å¯¾å¿œç‰ˆ)
# ==========================================
class BoatRaceScraperV5:
    BASE_URL = "https://www.boatrace.jp/owpc/pc/race/beforeinfo"
    LIST_URL = "https://www.boatrace.jp/owpc/pc/race/racelist"
    INDEX_URL = "https://www.boatrace.jp/owpc/pc/race/index"
    
    COURSE_MAP = {
        "æ¡ç”Ÿ": "01", "æˆ¸ç”°": "02", "æ±Ÿæˆ¸å·": "03", "å¹³å’Œå³¶": "04", "å¤šæ‘©å·": "05",
        "æµœåæ¹–": "06", "è’²éƒ¡": "07", "å¸¸æ»‘": "08", "æ´¥": "09", "ä¸‰å›½": "10",
        "ã³ã‚ã“": "11", "ä½ä¹‹æ±Ÿ": "12", "å°¼å´": "13", "é³´é–€": "14", "ä¸¸äº€": "15",
        "å…å³¶": "16", "å®®å³¶": "17", "å¾³å±±": "18", "ä¸‹é–¢": "19", "è‹¥æ¾": "20",
        "èŠ¦å±‹": "21", "ç¦å²¡": "22", "å”æ´¥": "23", "å¤§æ‘": "24"
    }

    def __init__(self):
        self.headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    def _get_soup(self, url, retries=2):
        for i in range(retries):
            try:
                res = requests.get(url, headers=self.headers, timeout=10)
                res.raise_for_status()
                return BeautifulSoup(res.content, "html.parser")
            except:
                time.sleep(1)
                continue
        return None

    def fetch_active_courses(self, date_str):
        soup = self._get_soup(f"{self.INDEX_URL}?hd={date_str}")
        if not soup: return []
        active_courses = []
        inv_map = {v: k for k, v in self.COURSE_MAP.items()}
        for link in soup.select("a[href*='jcd=']"):
            m = re.search(r"jcd=(\d{2})", link['href'])
            if m and m.group(1) in inv_map:
                active_courses.append(inv_map[m.group(1)])
        return sorted(list(set(active_courses)))

    def get_target_races_for_course(self, course, date_str, now_dt):
        jcd = self.COURSE_MAP[course]
        url = f"{self.LIST_URL}?jcd={jcd}&hd={date_str}"
        soup = self._get_soup(url)
        targets = []
        if not soup: return []
        bodies = soup.select("tbody") 
        current_r = 1
        for b in bodies:
            text = b.get_text().replace("\n", " ")
            m = re.search(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", text)
            if m:
                time_str = m.group(1).zfill(5)
                try:
                    race_dt = datetime.strptime(f"{date_str} {time_str}", "%Y%m%d %H:%M").replace(tzinfo=JST)
                    minutes = (race_dt - now_dt).total_seconds() / 60
                    # å±•ç¤ºç¢ºå®šã‹ã‚‰ç· åˆ‡å‰ã¾ã§ã®10ã€œ25åˆ†å‰ã‚’ç‹™ã„æ’ƒã¡
                    if 10 <= minutes <= 25: targets.append(current_r)
                except: pass
            current_r += 1
            if current_r > 12: break
        return targets

    def fetch_race_data(self, course, rno, date_str):
        jcd = self.COURSE_MAP[course]
        try:
            # 1. å‡ºèµ°è¡¨ã‹ã‚‰ç´šåˆ¥ãƒ»å‹ç‡ã‚’å–å¾—
            soup_list = self._get_soup(f"{self.LIST_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_list: return None
            
            deadline = "00:00"
            m_time = re.search(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", soup_list.get_text())
            if m_time: deadline = m_time.group(1).zfill(5)
            
            bodies = soup_list.select("tbody.is-fs12") or soup_list.select("tbody")
            boat_info = {}
            for i in range(1, 7):
                rank, win_rate = "B2", 0.0
                for b in bodies:
                    if b.select_one(f".is-ladder{i}") or str(i) in b.text[:5]:
                        r_m = re.search(r"([AB][12])", b.get_text())
                        if r_m: rank = r_m.group(1)
                        rates = re.findall(r"(\d\.\d{2})", b.get_text())
                        if rates: win_rate = float(rates[0])
                        break
                boat_info[i] = {"rank": rank, "win_rate": win_rate}

            # 2. ç›´å‰æƒ…å ±ï¼ˆå±•ç¤ºãƒ»æ°—è±¡ï¼‰
            soup_info = self._get_soup(f"{self.BASE_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_info or "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in soup_info.text: return None

            weather = soup_info.select_one(".weather1")
            wind, wave = 0, 0
            if weather:
                w_m = re.search(r"é¢¨é€Ÿ.*?(\d+)m", weather.text)
                h_m = re.search(r"æ³¢é«˜.*?(\d+)cm", weather.text)
                wind, wave = (int(w_m.group(1)), int(h_m.group(1))) if w_m else (0, 0)

            ex_rows = soup_info.select_one(".is-w748").select("tbody")
            data = {"wind_speed": wind, "wave": wave, "deadline": deadline, "rank_1": boat_info[1]["rank"]}
            for i in range(1, 7):
                tds = ex_rows[i-1].select("td")
                ex_val = tds[4].text.strip()
                data[f"ex_time_{i}"] = float(ex_val) if ex_val and ex_val[0].isdigit() else 6.80
                st_text = tds[2].select_one(".is-fs11").text.strip() if tds[2].select_one(".is-fs11") else ".15"
                data[f"st_{i}"] = float("0"+re.search(r"(\.\d+)", st_text).group(1)) if re.search(r"(\.\d+)", st_text) else 0.15
                data[f"rank_{i}"] = boat_info[i]["rank"]
                data[f"win_rate_{i}"] = boat_info[i]["win_rate"]
            return data
        except: return None

# ==========================================
# 2. äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯ (ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ»ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œ)
# ==========================================
def predict_single(model, config, scraper, course, rno, date_str):
    try:
        data = scraper.fetch_race_data(course, rno, date_str)
        if not data: return None, -1
        
        # ç‰¹å¾´é‡ç”Ÿæˆ
        ex_cols = [f"ex_time_{i}" for i in range(1, 7)]
        ex_vals = [data[c] for c in ex_cols]
        ex_mean = np.mean(ex_vals)
        rank_map = {"A1": 4, "A2": 3, "B1": 2, "B2": 1}
        input_dict = {"wind_speed": data["wind_speed"], "wave": data["wave"]}
        ex_ranks = pd.Series(ex_vals).rank(method="min").tolist()
        for i in range(1, 7):
            idx = i - 1
            input_dict[f"rank_val_{i}"] = rank_map.get(data[f"rank_{i}"], 2)
            input_dict[f"win_rate_{i}"] = data[f"win_rate_{i}"]
            input_dict[f"ex_time_{i}"] = data[f"ex_time_{i}"]
            input_dict[f"ex_diff_{i}"] = data[f"ex_time_{i}"] - ex_mean
            input_dict[f"ex_rank_{i}"] = ex_ranks[idx]
            input_dict[f"st_{i}"] = data[f"st_{i}"]
        input_dict["is_debuff_1"] = 1 if (input_dict["rank_val_1"] <= 2 and input_dict["ex_rank_1"] >= 4) else 0
        
        # äºˆæ¸¬å®Ÿè¡Œ
        input_df = pd.DataFrame([input_dict])[config["features"]]
        probs = model.predict(input_df)[0]
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ (1å·è‰‡ã‚’é™¤å¤–ã—ãŸå‹ç‡)
        boat_probs = {i+1: p for i, p in enumerate(probs)}
        ranking = sorted({k: v for k, v in boat_probs.items() if k != 1}.items(), key=lambda x: x[1], reverse=True)
        
        top1, top2, top3 = ranking[0], ranking[1], ranking[2]
        in_jump_prob = 1 - probs[0]
        
        # é–¾å€¤åˆ¤å®š (ROI 150%è¶…ãˆè¨­å®š)
        strategy = ""
        if in_jump_prob >= 0.55:
            if top1[1] >= 0.35: strategy = "FOCUS"
            elif top1[1] >= 0.25: strategy = "STANDARD"
            else: strategy = "WIDE"
        
        if not strategy: return None, 0

        # ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        if strategy == "FOCUS":
            bet_msg = f"{top1[0]} - {top2[0]}{top3[0]} - å…¨"
        elif strategy == "STANDARD":
            bet_msg = f"{top1[0]}{top2[0]} - {top1[0]}{top2[0]}{top3[0]} - å…¨"
        else:
            bet_msg = f"{top1[0]},{top2[0]},{top3[0]} BOXæ¨å¥¨ (1æŠœãåºƒåŸŸ)"

        res_dict = {
            "å ´å": course, "ãƒ¬ãƒ¼ã‚¹": f"{rno}R", "ç· åˆ‡": data['deadline'],
            "ã‚¤ãƒ³é£›ã³ç‡": in_jump_prob, "æˆ¦ç•¥": strategy,
            "1ä½": top1, "2ä½": top2, "3ä½": top3,
            "æ ¹æ‹ ": f"1å·è‰‡ç´šåˆ¥:{data['rank_1']} / å±•ç¤º:{int(input_dict['ex_rank_1'])}ä½",
            "è²·ã„ç›®": bet_msg
        }
        return res_dict, 1
    except: return None, -2

# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ (è‡ªå‹•ãƒ‘ãƒˆãƒ­ãƒ¼ãƒ«)
# ==========================================
def run_live_patrol():
    if not MODEL_PATH.exists():
        print("Error: Model file not found.")
        return

    with open(MODEL_PATH, "rb") as f: model = pickle.load(f)
    with open(CONFIG_PATH, "rb") as f: config = pickle.load(f)

    scraper = BoatRaceScraperV5()
    now = datetime.now(JST)
    date_str = now.strftime("%Y%m%d")
    
    courses = scraper.fetch_active_courses(date_str)
    
    for course in courses:
        targets = scraper.get_target_races_for_course(course, date_str, now)
        for rno in targets:
            race_id = f"{date_str}_{course}_{rno}"
            if is_already_notified(race_id): continue

            print(f"Analyzing {course} {rno}R...")
            res, status = predict_single(model, config, scraper, course, rno, date_str)
            
            if status == 1:
                content = f"ğŸ¯ ** æŠ•è³‡ãƒãƒ£ãƒ³ã‚¹åˆ°æ¥ï¼**\nğŸ“ **{res['å ´å']} {res['ãƒ¬ãƒ¼ã‚¹']}** (ç· åˆ‡ {res['ç· åˆ‡']})\n"
                content += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ”¥ æˆ¦ç•¥: **{res['æˆ¦ç•¥']}**\nğŸ˜± ã‚¤ãƒ³é£›ã³ç‡: `{res['ã‚¤ãƒ³é£›ã³ç‡']:.1%}`\n\n"
                content += f"ğŸ“Š **AIå‹ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚° (1æŠœã)**\nğŸ¥‡ **{res['1ä½'][0]}å·è‰‡**: `{res['1ä½'][1]:.1%}`\nğŸ¥ˆ **{res['2ä½'][0]}å·è‰‡**: `{res['2ä½'][1]:.1%}`\nğŸ¥‰ **{res['3ä½'][0]}å·è‰‡**: `{res['3ä½'][1]:.1%}`\n\n"
                content += f"ğŸ“ æ ¹æ‹ : {res['æ ¹æ‹ ']}\nğŸ’° æ¨å¥¨: `{res['è²·ã„ç›®']}`\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
                
                if DISCORD_WEBHOOK_URL:
                    requests.post(DISCORD_WEBHOOK_URL, json={"content": content})
                    print(f"Sent notification for {race_id}")
                save_notified_race(race_id)
            time.sleep(1)

if __name__ == "__main__":
    run_live_patrol()
