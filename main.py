import os
import pandas as pd
import numpy as np
import pickle
import re
import requests
import time
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime, timedelta, timezone

# ==========================================
# è¨­å®š
# ==========================================
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
JST = timezone(timedelta(hours=9), 'JST')

# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (GitHub Actionsç­‰ã§ã®å‹•ä½œã‚’æƒ³å®šã—ç›¸å¯¾ãƒ‘ã‚¹ã§å®šç¾©)
BASE_DIR = Path(__file__).resolve().parent
MODEL_PATH = BASE_DIR / "final_model_v4.pkl"
CONFIG_PATH = BASE_DIR / "model_config_v4.pkl"

# é€šçŸ¥æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
LOG_FILE = Path("notified_races.log")

# ==========================================
# é‡è¤‡é€šçŸ¥é˜²æ­¢ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
def is_already_notified(race_id):
    if not LOG_FILE.exists():
        return False
    with open(LOG_FILE, "r") as f:
        notified_races = f.read().splitlines()
    return race_id in notified_races

def save_notified_race(race_id):
    with open(LOG_FILE, "a") as f:
        f.write(race_id + "\n")

# ==========================================
# ==========================================
# 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ (v5: æŒ‡ç´‹å½è£…ãƒ»Refererå¼·åŒ–ç‰ˆ)
# ==========================================
class BoatRaceScraperV5:
    BASE_URL = "https://www.boatrace.jp/owpc/pc/race/beforeinfo"
    LIST_URL = "https://www.boatrace.jp/owpc/pc/race/racelist"
    INDEX_URL = "https://www.boatrace.jp/owpc/pc/race/index"
    
    COURSE_MAP = {
        "æ¡ç”Ÿ": "01", "æˆ¸ç”°": "02", "æ±Ÿæˆ¸å·": "03", "å¹³å’Œå³¶": "04", "å¤šæ‘©å·": "05",
        "æµœåæ¹–": "06", "è’²éƒ¡": "07", "å¸¸æ»‘": "08", "æ´¥": "09", "ä¸‰å›½": "10",
        "ã³ã‚ã“": "11", "ä½ä¹‹æ±Ÿ": "12", "å°¼å´": "13", "é³´é–€": "14", "ä¸¸äº€": "15",
        "å…å³¶": "16", "å®®å³¶": "17", "å¾³å±±": "18", "ä¸‹é–¢": "19", "è‹¥æ¾": "20",
        "èŠ¦å±‹": "21", "ç¦å²¡": "22", "å”æ´¥": "23", "å¤§æ‘": "24"
    }

    def __init__(self):
        # ã‚ˆã‚Šãƒ–ãƒ©ã‚¦ã‚¶ã«è¿‘ã„ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.course_links = {} # {course_name: list_url}
        self.date_str = ""
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ– (Warm-up)
        try:
            self.session.get("https://www.boatrace.jp/", timeout=15)
        except: pass

    def _get_soup(self, url, referer=None, retries=3):
        for i in range(retries):
            try:
                headers = {"Referer": referer} if referer else {}
                res = self.session.get(url, headers=headers, timeout=20)
                res.raise_for_status()
                return BeautifulSoup(res.content, "html.parser")
            except Exception as e:
                wait = (i + 1) * 3
                print(f"[{datetime.now(JST).strftime('%H:%M:%S')}] âš ï¸ Retry {i+1}/{retries}: {url} - {e}")
                time.sleep(wait)
        return None

    def fetch_active_courses(self, date_str):
        self.date_str = date_str
        print(f"[{datetime.now(JST).strftime('%H:%M:%S')}] ğŸ” Fetching active courses...")
        index_url = f"{self.INDEX_URL}?hd={date_str}"
        soup = self._get_soup(index_url, referer="https://www.boatrace.jp/")
        if not soup: return []
        
        self.course_links = {}
        active_courses = []
        inv_map = {v: k for k, v in self.COURSE_MAP.items()}
        
        # indexãƒšãƒ¼ã‚¸ã«ã‚ã‚‹å®Ÿéš›ã®ãƒªãƒ³ã‚¯(href)ã‚’æŠ½å‡ºã—ã¦ä¿å­˜ã™ã‚‹
        for link in soup.select("a[href*='jcd=']"):
            href = link.get('href', '')
            m = re.search(r"jcd=(\d{2})", href)
            if m and m.group(1) in inv_map:
                name = inv_map[m.group(1)]
                if href.startswith("/"):
                    href = "https://www.boatrace.jp" + href
                self.course_links[name] = href
                active_courses.append(name)
        
        return sorted(list(set(active_courses)))

    def get_target_races_for_course(self, course, date_str, now_dt):
        # è‡ªåˆ†ã§URLã‚’çµ„ã¿ç«‹ã¦ãšã€indexãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã—ãŸãƒªãƒ³ã‚¯ã‚’ãã®ã¾ã¾ä½¿ã†
        url = self.course_links.get(course)
        if not url:
            jcd = self.COURSE_MAP[course]
            url = f"{self.LIST_URL}?jcd={jcd}&hd={date_str}"
            
        index_url = f"{self.INDEX_URL}?hd={date_str}"
        soup = self._get_soup(url, referer=index_url)
        targets = []
        if not soup:
            print(f"  âŒ Failed to get race list for {course}")
            return []
        
        page_text = soup.get_text().replace("\n", " ").replace("\r", " ").strip()
        all_deadlines = re.findall(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", page_text)
        
        if not all_deadlines:
            # å–å¾—å¤±æ•—æ™‚ã«ã‚¿ã‚¤ãƒˆãƒ«ãªã©ã‚’è¡¨ç¤ºã—ã¦åŸå› ã‚’æ¢ã‚‹
            title = soup.title.string if soup.title else "No Title"
            print(f"  âš ï¸ No deadline found in {course}. (Title: {title})")
            return []

        for i, time_str in enumerate(all_deadlines):
            current_r = i + 1
            if current_r > 12: break
            try:
                race_dt = datetime.strptime(f"{date_str} {time_str.zfill(5)}", "%Y%m%d %H:%M").replace(tzinfo=JST)
                minutes = (race_dt - now_dt).total_seconds() / 60
                if 5 <= minutes <= 45:
                    print(f"  - {course} {current_r}R: ç· åˆ‡ã¾ã§ {minutes:.1f}åˆ† ({time_str})")
                if 5 <= minutes <= 35: 
                    targets.append(current_r)
            except Exception as e:
                print(f"  Error parsing time for {course} {current_r}R: {e}")
        return targets

    def fetch_race_data(self, course, rno, date_str):
        # ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã¸ã®å‚ç…§ã‚‚ä¿å­˜ã•ã‚ŒãŸã‚‚ã®ã‚’ä½¿ã†
        list_url = self.course_links.get(course, f"{self.LIST_URL}?jcd={self.COURSE_MAP[course]}&hd={date_str}")
        try:
            # å‡ºèµ°è¡¨(è©³ç´°)ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            race_list_url = f"{self.LIST_URL}?rno={rno}&jcd={self.COURSE_MAP[course]}&hd={date_str}"
            soup_list = self._get_soup(race_list_url, referer=list_url)
            if not soup_list: return None
            
            deadline_str = "00:00"
            m_time = re.search(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", soup_list.get_text())
            if m_time: deadline_str = m_time.group(1).zfill(5)
            
            # ç›´å‰æƒ…å ±ã®URL
            info_url = f"{self.BASE_URL}?rno={rno}&jcd={self.COURSE_MAP[course]}&hd={date_str}"
            soup_info = self._get_soup(info_url, referer=race_list_url)
            if not soup_info or "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in soup_info.text: return None
            
            bodies = soup_list.select("tbody.is-fs12") or soup_list.select("tbody")
            
            boat_info = {}
            for i in range(1, 7):
                rank, win_rate = "B2", 0.0
                for b in bodies:
                    is_boat_row = b.select_one(f".is-ladder{i}") or str(i) in b.text[:5]
                    if is_boat_row:
                        r_m = re.search(r"([AB][12])", b.get_text())
                        if r_m: rank = r_m.group(1)
                        rates = re.findall(r"(\d\.\d{2})", b.get_text())
                        if rates: win_rate = float(rates[0])
                        break
                boat_info[i] = {"rank": rank, "win_rate": win_rate}

            soup_info = self._get_soup(f"{self.BASE_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_info or "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in soup_info.text: return None

            weather = soup_info.select_one(".weather1")
            wind_speed, wave = 0, 0
            if weather:
                txt = weather.text
                w_m = re.search(r"é¢¨é€Ÿ.*?(\d+)m", txt)
                h_m = re.search(r"æ³¢é«˜.*?(\d+)cm", txt)
                if w_m: wind_speed = int(w_m.group(1))
                if h_m: wave = int(h_m.group(1))

            table = soup_info.select_one(".is-w748")
            if not table: return None
            rows = table.select("tbody")
            
            data = {"wind_speed": wind_speed, "wave": wave, "deadline": deadline_str}
            for i in range(1, 7):
                tds = rows[i-1].select("td")
                ex_val = tds[4].text.strip()
                data[f"ex_time_{i}"] = float(ex_val) if ex_val and ex_val[0].isdigit() else 6.80
                st_text = tds[2].select_one(".is-fs11").text.strip() if tds[2].select_one(".is-fs11") else ".15"
                data[f"st_{i}"] = float("0"+re.search(r"(\.\d+)", st_text).group(1)) if re.search(r"(\.\d+)", st_text) else 0.15
                data[f"rank_{i}"] = boat_info[i]["rank"]
                data[f"win_rate_{i}"] = boat_info[i]["win_rate"]

            return data
        except: return None

# ==========================================
# 2. äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
def predict_single(model, config, scraper, course, rno, date_str):
    try:
        data = scraper.fetch_race_data(course, rno, date_str)
        if not data: 
            print(f"  âš ï¸ Failed to fetch detail data for {course} {rno}R")
            return None, -1
        
        ex_cols = [f"ex_time_{i}" for i in range(1, 7)]
        ex_vals = [data[c] for c in ex_cols]
        ex_mean = np.mean(ex_vals)
        rank_map = {"A1": 4, "A2": 3, "B1": 2, "B2": 1}
        
        input_dict = {"wind_speed": data["wind_speed"], "wave": data["wave"]}
        ex_ranks = pd.Series(ex_vals).rank(method="min").tolist()
        
        for i in range(1, 7):
            idx = i - 1
            rv = rank_map.get(data[f"rank_{i}"], 2)
            input_dict[f"rank_val_{i}"] = rv
            input_dict[f"win_rate_{i}"] = data[f"win_rate_{i}"]
            input_dict[f"ex_time_{i}"] = data[f"ex_time_{i}"]
            input_dict[f"ex_diff_{i}"] = data[f"ex_time_{i}"] - ex_mean
            input_dict[f"ex_rank_{i}"] = ex_ranks[idx]
            input_dict[f"st_{i}"] = data[f"st_{i}"]
            
        input_dict["is_debuff_1"] = 1 if (input_dict["rank_val_1"] <= 2 and input_dict["ex_rank_1"] >= 4) else 0
        
        input_df = pd.DataFrame([input_dict])[config["features"]]
        probs = model.predict(input_df)[0]
        
        in_jump_prob = 1 - probs[0]
        ranking = sorted({i+1: p for i, p in enumerate(probs) if i > 0}.items(), key=lambda x: x[1], reverse=True)
        top1, top2, top3 = ranking[0], ranking[1], ranking[2]
        
        strategy = ""
        if in_jump_prob >= 0.55:
            if top1[1] >= 0.35: strategy = "FOCUS"
            elif top1[1] >= 0.25: strategy = "STANDARD"
            else: strategy = "WIDE"
        
        if not strategy: return None, 0

        res_dict = {
            "å ´å": course, "ãƒ¬ãƒ¼ã‚¹": f"{rno}R", "ç· åˆ‡": data['deadline'],
            "ã‚¤ãƒ³é£›ã³ç‡": in_jump_prob, "æˆ¦ç•¥": strategy,
            "1ä½": top1, "2ä½": top2, "3ä½": top3,
            "æ ¹æ‹ ": f"1å·è‰‡:{data['rank_1']} / å±•ç¤º:{int(input_dict['ex_rank_1'])}ä½",
            "è²·ã„ç›®": f"{top1[0]}-{top2[0]}{top3[0]}-å…¨" if strategy != "WIDE" else "1æŠœãBOXæ¨å¥¨"
        }
        return res_dict, 1
        
    except Exception as e:
        print(f"Error in prediction: {e}")
        return None, -2

# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ (ãƒ‘ãƒˆãƒ­ãƒ¼ãƒ«)
# ==========================================
def run_live_patrol():
    print(f"ğŸ‘® Smart Patrol Start: {datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S')}")
    
    if not MODEL_PATH.exists():
        print(f"âŒ Error: Model file not found at {MODEL_PATH}")
        return

    with open(MODEL_PATH, "rb") as f: model = pickle.load(f)
    with open(CONFIG_PATH, "rb") as f: config = pickle.load(f)
    print("âœ… Model loaded successfully.")

    scraper = BoatRaceScraperV5()
    now_jst = datetime.now(JST)
    date_str = now_jst.strftime("%Y%m%d")
    
    courses = scraper.fetch_active_courses(date_str)
    print(f"Active Courses: {courses}")
    
    hit_count = 0
    for course in courses:
        print(f"[{datetime.now(JST).strftime('%H:%M:%S')}] ğŸ Checking {course}...")
        targets = scraper.get_target_races_for_course(course, date_str, now_jst)
        
        if not targets:
            # print(f"  (No target races in {course})")
            pass
            
        for rno in targets:
            race_id = f"{date_str}_{course}_{rno}"
            
            # é€šçŸ¥æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
            if is_already_notified(race_id):
                print(f"  - {course} {rno}R: Already notified, skipping.")
                continue

            print(f"  - {course} {rno}R: Analyzing...")
            res, status = predict_single(model, config, scraper, course, rno, date_str)
            
            if status == 1:
                hit_count += 1
                # Discordé€šçŸ¥å‡¦ç† (ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’èª¿æ•´)
                content = f"ğŸ¯ **æŠ•è³‡ãƒãƒ£ãƒ³ã‚¹åˆ°æ¥ï¼**\nğŸ“ **{res['å ´å']} {res['ãƒ¬ãƒ¼ã‚¹']}** (ç· åˆ‡ {res['ç· åˆ‡']})\n"
                content += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ”¥ æˆ¦ç•¥: **{res['æˆ¦ç•¥']}**\nğŸ˜± ã‚¤ãƒ³é£›ã³ç‡: `{res['ã‚¤ãƒ³é£›ã³ç‡']:.1%}`\n\n"
                content += f"ğŸ“Š **AIå‹ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚° (1æŠœã)**\nğŸ¥‡ **{res['1ä½'][0]}å·è‰‡**: `{res['1ä½'][1]:.1%}`\nğŸ¥ˆ **{res['2ä½'][0]}å·è‰‡**: `{res['2ä½'][1]:.1%}`\nğŸ¥‰ **{res['3ä½'][0]}å·è‰‡**: `{res['3ä½'][1]:.1%}`\n\n"
                content += f"ğŸ“ æ ¹æ‹ : {res['æ ¹æ‹ ']}\nğŸ’° æ¨å¥¨: `{res['è²·ã„ç›®']}`\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
                
                if DISCORD_WEBHOOK_URL:
                    try:
                        requests.post(DISCORD_WEBHOOK_URL, json={"content": content}, timeout=15)
                        print(f"    âœ… Notification Sent for {race_id}")
                    except Exception as e:
                        print(f"    âŒ Discord Error: {e}")
                
                # é€šçŸ¥æ¸ˆã¿ãƒªã‚¹ãƒˆã«ä¿å­˜
                save_notified_race(race_id)
            time.sleep(1)

    print(f"ğŸ‘® Patrol Finished: Found {hit_count} hits.")

if __name__ == "__main__":
    run_live_patrol()
