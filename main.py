import os
import pandas as pd
import numpy as np
import pickle
import re
import requests
import time
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime, timedelta, timezone

# ==========================================
# è¨­å®š
# ==========================================
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
JST = timezone(timedelta(hours=9), 'JST')

MODEL_DIR = "boatrace/output_v4"

# ==========================================
# 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ (v5: å…¨è‰‡ã®ç´šåˆ¥ãƒ»å‹ç‡å¯¾å¿œ)
# ==========================================
class BoatRaceScraperV5:
    BASE_URL = "https://www.boatrace.jp/owpc/pc/race/beforeinfo"
    LIST_URL = "https://www.boatrace.jp/owpc/pc/race/racelist"
    INDEX_URL = "https://www.boatrace.jp/owpc/pc/race/index"
    
    COURSE_MAP = {
        "æ¡ç”Ÿ": "01", "æˆ¸ç”°": "02", "æ±Ÿæˆ¸å·": "03", "å¹³å’Œå³¶": "04", "å¤šæ‘©å·": "05",
        "æµœåæ¹–": "06", "è’²éƒ¡": "07", "å¸¸æ»‘": "08", "æ´¥": "09", "ä¸‰å›½": "10",
        "ã³ã‚ã“": "11", "ä½ä¹‹æ±Ÿ": "12", "å°¼å´": "13", "é³´é–€": "14", "ä¸¸äº€": "15",
        "å…å³¶": "16", "å®®å³¶": "17", "å¾³å±±": "18", "ä¸‹é–¢": "19", "è‹¥æ¾": "20",
        "èŠ¦å±‹": "21", "ç¦å²¡": "22", "å”æ´¥": "23", "å¤§æ‘": "24"
    }

    def __init__(self):
        self.headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    def _get_soup(self, url, retries=2):
        for i in range(retries):
            try:
                res = requests.get(url, headers=self.headers, timeout=10)
                res.raise_for_status()
                return BeautifulSoup(res.content, "html.parser")
            except:
                time.sleep(1)
                continue
        return None

    def fetch_active_courses(self, date_str):
        soup = self._get_soup(f"{self.INDEX_URL}?hd={date_str}")
        if not soup: return []
        active_courses = []
        inv_map = {v: k for k, v in self.COURSE_MAP.items()}
        for link in soup.select("a[href*='jcd=']"):
            m = re.search(r"jcd=(\d{2})", link['href'])
            if m and m.group(1) in inv_map:
                active_courses.append(inv_map[m.group(1)])
        return sorted(list(set(active_courses)))

    def get_target_races_for_course(self, course, date_str, now_dt):
        jcd = self.COURSE_MAP[course]
        url = f"{self.LIST_URL}?jcd={jcd}&hd={date_str}"
        soup = self._get_soup(url)
        targets = []
        if not soup: return []
        
        bodies = soup.select("tbody") 
        current_r = 1
        for b in bodies:
            text = b.get_text().replace("\n", " ")
            m = re.search(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", text)
            if m:
                time_str = m.group(1).zfill(5)
                race_dt_str = f"{date_str} {time_str}"
                try:
                    race_dt = datetime.strptime(race_dt_str, "%Y%m%d %H:%M").replace(tzinfo=JST)
                    diff = race_dt - now_dt
                    minutes = diff.total_seconds() / 60
                    if 10 <= minutes <= 40: # ç· åˆ‡10ã€œ40åˆ†å‰ã‚’å¯¾è±¡
                        targets.append(current_r)
                except: pass
            current_r += 1
            if current_r > 12: break
        return targets

    def fetch_race_data(self, course, rno, date_str):
        jcd = self.COURSE_MAP[course]
        try:
            # 1. å‡ºèµ°è¡¨
            soup_list = self._get_soup(f"{self.LIST_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_list: return None
            
            # ç· åˆ‡æ™‚åˆ»
            deadline_str = "00:00"
            m_time = re.search(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", soup_list.get_text())
            if m_time: deadline_str = m_time.group(1).zfill(5)
            
            # å„è‰‡ã®ç´šåˆ¥ã¨å‹ç‡
            bodies = soup_list.select("tbody.is-fs12")
            if len(bodies) < 6: bodies = soup_list.select("tbody") # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            # è‰‡ç•ªã”ã¨ã®æƒ…å ±ã‚’æŠ½å‡º
            boat_info = {}
            for i in range(1, 7):
                target_body = None
                for b in bodies:
                    if str(i) in b.text[:10]: # è‰‡ç•ªãŒãƒ†ã‚­ã‚¹ãƒˆã®å…ˆé ­ä»˜è¿‘ã«ã‚ã‚‹ã‹
                        target_body = b
                        break
                
                rank, win_rate = "B2", 0.0
                if target_body:
                    r_m = re.search(r"/ ([AB][12])", target_body.text)
                    if r_m: rank = r_m.group(1)
                    rates = re.findall(r"(\d\.\d{2})", target_body.get_text())
                    if rates: win_rate = float(rates[0])
                boat_info[i] = {"rank": rank, "win_rate": win_rate}

            # 2. ç›´å‰æƒ…å ±
            soup_info = self._get_soup(f"{self.BASE_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_info or "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in soup_info.text: return None

            weather = soup_info.select_one(".weather1")
            wind_speed, wave = 0, 0
            if weather:
                txt = weather.text
                w_m = re.search(r"é¢¨é€Ÿ.*?(\d+)m", txt)
                h_m = re.search(r"æ³¢é«˜.*?(\d+)cm", txt)
                if w_m: wind_speed, wave = int(w_m.group(1)), int(h_m.group(1))

            table = soup_info.select_one(".is-w748")
            if not table: return None
            rows = table.select("tbody")
            
            data = {"wind_speed": wind_speed, "wave": wave, "deadline": deadline_str}
            for i in range(1, 7):
                tds = rows[i-1].select("td")
                if len(tds) < 5: return None
                ex_val = tds[4].text.strip()
                data[f"ex_time_{i}"] = float(ex_val) if ex_val else 6.80
                st_text = tds[2].select_one(".is-fs11").text.strip() if tds[2].select_one(".is-fs11") else "0.00"
                data[f"st_{i}"] = float("0"+re.search(r"(\.\d+)", st_text).group(1)) if re.search(r"(\.\d+)", st_text) else 0.15
                data[f"rank_{i}"] = boat_info[i]["rank"]
                data[f"win_rate_{i}"] = boat_info[i]["win_rate"]

            return data
        except: return None

# ==========================================
# 2. äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
def predict_single(model, config, scraper, course, rno, date_str):
    try:
        data = scraper.fetch_race_data(course, rno, date_str)
        if not data: return None, -1
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° (v4åŒç­‰)
        ex_cols = [f"ex_time_{i}" for i in range(1, 7)]
        ex_vals = [data[c] for c in ex_cols]
        ex_mean = np.mean(ex_vals)
        rank_map = {"A1": 4, "A2": 3, "B1": 2, "B2": 1}
        
        input_dict = {"wind_speed": data["wind_speed"], "wave": data["wave"]}
        
        # å„è‰‡ã®ç‰¹å¾´é‡
        ex_ranks = pd.Series(ex_vals).rank(method="min").tolist()
        for i in range(1, 7):
            idx = i - 1
            rv = rank_map.get(data[f"rank_{i}"], 2)
            input_dict[f"rank_val_{i}"] = rv
            input_dict[f"win_rate_{i}"] = data[f"win_rate_{i}"]
            input_dict[f"ex_time_{i}"] = data[f"ex_time_{i}"]
            input_dict[f"ex_diff_{i}"] = data[f"ex_time_{i}"] - ex_mean
            input_dict[f"ex_rank_{i}"] = ex_ranks[idx]
            input_dict[f"st_{i}"] = data[f"st_{i}"]
            
        # is_debuff_1 ã®è¨ˆç®—
        is_debuff_1 = 1 if (input_dict["rank_val_1"] <= 2 and input_dict["ex_rank_1"] >= 4) else 0
        input_dict["is_debuff_1"] = is_debuff_1
        
        # äºˆæ¸¬
        input_df = pd.DataFrame([input_dict])[config["features"]]
        probs = model.predict(input_df)[0]
        
        in_win_prob = probs[0]
        in_jump_prob = 1 - in_win_prob
        
        # ä»–è‰‡ã®åˆ†æ
        other_probs = probs[1:]
        top_other_idx = np.argmax(other_probs)
        top_other_boat = top_other_idx + 2
        top_other_prob = other_probs[top_other_idx]
        
        # æˆ¦ç•¥åˆ¤å®š
        strategy = ""
        if in_jump_prob >= 0.55:
            if top_other_prob >= 0.35: strategy = "FOCUS"
            elif top_other_prob >= 0.25: strategy = "STANDARD"
            else: strategy = "WIDE"
        
        if not strategy: return None, 0

        res_dict = {
            "å ´å": course, "ãƒ¬ãƒ¼ã‚¹": f"{rno}R", "ç· åˆ‡": data['deadline'],
            "ã‚¤ãƒ³é£›ã³ç‡": in_jump_prob, "æˆ¦ç•¥": strategy,
            "è»¸è‰‡": f"{top_other_boat}å·è‰‡", "è»¸ç¢ºç‡": top_other_prob,
            "æ ¹æ‹ ": f"1å·è‰‡ç´šåˆ¥:{data['rank_1']} / å±•ç¤º:{int(input_dict['ex_rank_1'])}ä½",
            "è²·ã„ç›®": f"{top_other_boat}-å…¨-å…¨" if strategy != "WIDE" else "1æŠœãBOXæ¨å¥¨"
        }
        return res_dict, 1
        
    except Exception as e:
        print(f"Error in predict_single: {e}")
        return None, -2

# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==========================================
def run_live_patrol():
    print("ğŸš€ Starting...")
    
    model_path = "final_model_v4.pkl"
    config_path = "model_config_v4.pkl"
    
    if not model_path.exists():
        print(f"Error: Model files not found at {model_path}")
        return

    with open(model_path, "rb") as f: model = pickle.load(f)
    with open(config_path, "rb") as f: config = pickle.load(f)

    scraper = BoatRaceScraperV5()
    now_jst = datetime.now(JST)
    date_str = now_jst.strftime("%Y%m%d")
    
    courses = scraper.fetch_active_courses(date_str)
    if not courses:
        print("No races today.")
        return
        
    hits = []
    for course in courses:
        targets = scraper.get_target_races_for_course(course, date_str, now_jst)
        for rno in targets:
            print(f"Analyzing {course} {rno}R...")
            res, status = predict_single(model, config, scraper, course, rno, date_str)
            if status == 1:
                hits.append(res)
            time.sleep(1)

    if hits and DISCORD_WEBHOOK_URL:
        for r in hits:
            content = f"ğŸ¯ ** æŠ•è³‡ãƒãƒ£ãƒ³ã‚¹åˆ°æ¥ï¼**\n"
            content += f"ğŸ“ **{r['å ´å']} {r['ãƒ¬ãƒ¼ã‚¹']}** (ç· åˆ‡ {r['ç· åˆ‡']})\n"
            content += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            content += f"ğŸ”¥ æˆ¦ç•¥: **{r['æˆ¦ç•¥']}**\n"
            content += f"ğŸ˜± ã‚¤ãƒ³é£›ã³ç¢ºç‡: `{r['ã‚¤ãƒ³é£›ã³ç‡']:.1%}`\n"
            content += f"ğŸ† æ³¨ç›®è»¸è‰‡: **{r['è»¸è‰‡']}** (å‹ç‡äºˆæ¸¬: `{r['è»¸ç¢ºç‡']:.1%}`)\n"
            content += f"ğŸ“ æ ¹æ‹ : {r['æ ¹æ‹ ']}\n"
            content += f"ğŸ’° æ¨å¥¨: `{r['è²·ã„ç›®']}`\n"
            content += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            requests.post(DISCORD_WEBHOOK_URL, json={"content": content})
            print(f"Sent notification for {r['å ´å']} {r['ãƒ¬ãƒ¼ã‚¹']}")

if __name__ == "__main__":
    run_live_patrol()
