import os
import pandas as pd
import numpy as np
import pickle
import re
import requests
import time
import concurrent.futures
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime, timedelta, timezone

# ==========================================
# è¨­å®š
# ==========================================
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
JST = timezone(timedelta(hours=9), 'JST')

# ==========================================
# 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
# ==========================================
class BoatRaceScraperV4:
    BASE_URL = "https://www.boatrace.jp/owpc/pc/race/beforeinfo"
    LIST_URL = "https://www.boatrace.jp/owpc/pc/race/racelist"
    INDEX_URL = "https://www.boatrace.jp/owpc/pc/race/index"
    
    COURSE_MAP = {
        "æ¡ç”Ÿ": "01", "æˆ¸ç”°": "02", "æ±Ÿæˆ¸å·": "03", "å¹³å’Œå³¶": "04", "å¤šæ‘©å·": "05",
        "æµœåæ¹–": "06", "è’²éƒ¡": "07", "å¸¸æ»‘": "08", "æ´¥": "09", "ä¸‰å›½": "10",
        "ã³ã‚ã“": "11", "ä½ä¹‹æ±Ÿ": "12", "å°¼å´": "13", "é³´é–€": "14", "ä¸¸äº€": "15",
        "å…å³¶": "16", "å®®å³¶": "17", "å¾³å±±": "18", "ä¸‹é–¢": "19", "è‹¥æ¾": "20",
        "èŠ¦å±‹": "21", "ç¦å²¡": "22", "å”æ´¥": "23", "å¤§æ‘": "24"
    }

    def __init__(self):
        self.headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    def _get_soup(self, url, retries=2):
        for i in range(retries):
            try:
                res = requests.get(url, headers=self.headers, timeout=10)
                res.raise_for_status()
                return BeautifulSoup(res.content, "html.parser")
            except:
                time.sleep(1) # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å°‘ã—å¾…ã¤
                continue
        return None

    def fetch_active_courses(self, date_str):
        soup = self._get_soup(f"{self.INDEX_URL}?hd={date_str}")
        if not soup: return []
        active_courses = []
        inv_map = {v: k for k, v in self.COURSE_MAP.items()}
        for link in soup.select("a[href*='jcd=']"):
            m = re.search(r"jcd=(\d{2})", link['href'])
            if m and m.group(1) in inv_map:
                active_courses.append(inv_map[m.group(1)])
        return sorted(list(set(active_courses)))

    # æ™‚åˆ»è¡¨ã ã‘ã‚’å…ˆã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹è»½é‡ãƒ¡ã‚½ãƒƒãƒ‰
    def get_target_races_for_course(self, course, date_str, now_dt):
        jcd = self.COURSE_MAP[course]
        url = f"{self.LIST_URL}?jcd={jcd}&hd={date_str}"
        soup = self._get_soup(url)
        targets = []
        
        if not soup: return []

        # ãƒšãƒ¼ã‚¸å†…ã®å…¨ãƒ¬ãƒ¼ã‚¹ã®ç· åˆ‡æ™‚åˆ»ã‚’æ¢ã™
        # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‹ã‚‰æ™‚åˆ»ã‚’æŠ½å‡º
        # é€šå¸¸ã€racelistãƒšãƒ¼ã‚¸ã®å„Rã®ãƒ˜ãƒƒãƒ€ãƒ¼ä»˜è¿‘ã«æ™‚åˆ»ãŒã‚ã‚‹
        # ç°¡æ˜“çš„ã«ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰ "1R ... 10:52" ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ã®ã¯å›°é›£ãªãŸã‚
        # HTMLæ§‹é€ ï¼ˆtbodyï¼‰ã‹ã‚‰é †ç•ªã«æ™‚é–“ã‚’æŠœã
        
        bodies = soup.select("tbody") 
        # å‡ºèµ°è¡¨ã¯é€šå¸¸12å€‹ã®tbodyã§æ§‹æˆã•ã‚Œã‚‹ (Rã”ã¨ã®å¡Š)
        
        current_r = 1
        for b in bodies:
            text = b.get_text().replace("\n", " ")
            # "ç· åˆ‡äºˆå®š 10:30" ã‚’æ¢ã™
            m = re.search(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", text)
            if m:
                time_str = m.group(1).zfill(5)
                race_dt_str = f"{date_str} {time_str}"
                try:
                    race_dt = datetime.strptime(race_dt_str, "%Y%m%d %H:%M").replace(tzinfo=JST)
                    diff = race_dt - now_dt
                    minutes = diff.total_seconds() / 60
                    
                    # ã€é‡è¦ã€‘ ã“ã“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼
                    # ç· åˆ‡ã¾ã§ 10åˆ†ã€œ35åˆ† ã®ãƒ¬ãƒ¼ã‚¹ã ã‘ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    if 10 <= minutes <= 35:
                        targets.append(current_r)
                except:
                    pass
            current_r += 1
            if current_r > 12: break
            
        return targets

    def fetch_race_data(self, course, rno, date_str):
        jcd = self.COURSE_MAP[course]
        try:
            # 1. å‡ºèµ°è¡¨
            soup_list = self._get_soup(f"{self.LIST_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_list: return None
            
            deadline_str = "00:00"
            text_full = soup_list.get_text()
            match_time = re.search(r"ç· åˆ‡äºˆå®š.*?(\d{1,2}:\d{2})", text_full)
            if match_time: deadline_str = match_time.group(1).zfill(5)
            
            bodies = soup_list.select("tbody.is-fs12")
            if not bodies: bodies = soup_list.select("tbody")
            
            row1 = None
            for b_idx, b in enumerate(bodies):
                if "ï¼‘" in b.text[:10]: row1 = b; break
            if not row1: return None

            rank_1, win_rate_1 = "B2", 0.0
            rank_match = re.search(r"/ ([AB][12])", row1.text)
            if rank_match: rank_1 = rank_match.group(1)
            
            td_texts = [td.text.strip().replace("\n", " ") for td in row1.find_all("td")]
            all_rates = re.findall(r"(\d\.\d{2})", " ".join(td_texts))
            if all_rates: win_rate_1 = float(all_rates[0])

            # 2. ç›´å‰æƒ…å ±
            soup_info = self._get_soup(f"{self.BASE_URL}?rno={rno}&jcd={jcd}&hd={date_str}")
            if not soup_info or "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in soup_info.text: return None

            weather = soup_info.select_one(".weather1")
            wind_speed, wave = 0, 0
            if weather:
                txt = weather.text
                w_m = re.search(r"é¢¨é€Ÿ.*?(\d+)m", txt)
                h_m = re.search(r"æ³¢é«˜.*?(\d+)cm", txt)
                if w_m: wind_speed = int(w_m.group(1))
                if h_m: wave = int(h_m.group(1))

            table = soup_info.select_one(".is-w748")
            if not table: return None
            rows = table.select("tbody")
            ex_times, st_list = [], []
            for i in range(6):
                tds = rows[i].select("td")
                if len(tds) < 5 or not tds[4].text.strip(): return None
                ex_times.append(float(tds[4].text.strip()))
                st_text = tds[2].select_one(".is-fs11").text.strip() if tds[2].select_one(".is-fs11") else "0.00"
                st_list.append(float(re.search(r"(\.\d+)", st_text).group(1)) if re.search(r"(\.\d+)", st_text) else 0.0)

            ex_rank = pd.Series(ex_times).rank(method="min").tolist()
            
            data = {
                "wind_speed": wind_speed, "wave": wave, 
                "ex_rank_1": ex_rank[0], "rank_1": rank_1, "win_rate_1": win_rate_1,
                "deadline": deadline_str 
            }
            for i in range(6):
                data[f"st_{i+1}"] = st_list[i]
                data[f"ex_time_{i+1}"] = ex_times[i]
            return data
        except: return None

# ==========================================
# 2. äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯ (ã‚¹ãƒŠã‚¤ãƒ‘ãƒ¼ãƒ¢ãƒ¼ãƒ‰)
# ==========================================
def predict_single(model, config, scraper, course, rno, date_str):
    try:
        race_data = scraper.fetch_race_data(course, rno, date_str)
        if not race_data: return None, -1
        
        # ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        rank_map = {"A1": 4, "A2": 3, "B1": 2, "B2": 1}
        rank_val_1 = rank_map.get(race_data["rank_1"], 2)
        # 1å·è‰‡ãŒä½ç´šåˆ¥ã‹ã¤å±•ç¤ºãŒæ‚ªã„(4ä½ä»¥ä¸‹)å ´åˆã«ãƒ‡ãƒãƒ•åˆ¤å®š
        is_debuff_1 = 1 if (rank_val_1 <= 2 and race_data["ex_rank_1"] >= 4) else 0
        
        input_data = race_data.copy()
        input_data["rank_val_1"] = rank_val_1
        input_data["is_debuff_1"] = is_debuff_1
        
        # äºˆæ¸¬å®Ÿè¡Œ (ã‚¤ãƒ³é£›ã³ç¢ºç‡ã‚’ç®—å‡º)
        input_df = pd.DataFrame([input_data])[config["features"]]
        prob = model.predict(input_df)[0]
        
        # ã‚¹ãƒŠã‚¤ãƒ‘ãƒ¼(è»¸)ã®é¸å®š: 2ã€œ6å·è‰‡ã®ä¸­ã§å±•ç¤ºã‚¿ã‚¤ãƒ ãŒæœ€ã‚‚é€Ÿã„è‰‡
        ex_times_26 = {i: race_data[f"ex_time_{i}"] for i in range(2, 7)}
        sniper_boat = min(ex_times_26, key=ex_times_26.get)
        
        # æ ¹æ‹ ã®æ•´ç†
        reason = []
        if is_debuff_1: reason.append("åœ°åŠ›ãƒ‡ãƒãƒ•(Bç´š)")
        if race_data["ex_rank_1"] >= 5: reason.append(f"1å·è‰‡å±•ç¤º{int(race_data['ex_rank_1'])}ä½(è‡´å‘½çš„)")
        if race_data["wind_speed"] >= 5: reason.append(f"å¼·é¢¨({race_data['wind_speed']}m)")
        
        reason_str = " / ".join(reason) if reason else "å±•ç¤ºãƒ»ç´šåˆ¥ãƒãƒ©ãƒ³ã‚¹å´©å£Š"

        res_dict = {
            "å ´å": course, "ãƒ¬ãƒ¼ã‚¹": f"{rno}R", "ç· åˆ‡": race_data['deadline'],
            "ç¢ºç‡": prob, # åˆ¤å®šç”¨ã«æ•°å€¤ã§æŒã¤
            "ã‚¹ãƒŠã‚¤ãƒ‘ãƒ¼": f"{sniper_boat}å·è‰‡",
            "1ç´šåˆ¥": race_data["rank_1"],
            "æ ¹æ‹ ": reason_str,
            "è²·ã„ç›®": f"{sniper_boat}-å…¨-å…¨ (ä¸‡èˆŸç‹™ã„)"
        }

        # ã—ãã„å€¤ï¼ˆãƒœãƒ¼ãƒ€ãƒ¼ 0.570ï¼‰ã‚’è¶…ãˆã¦ã„ã‚‹ã‹åˆ¤å®š
        # configã«ç„¡ã„å ´åˆã¯ç›´æ¥ 0.570 ã‚’ä½¿ç”¨
        border = config.get("best_threshold", 0.570)
        if prob >= border:
            return res_dict, 1
        return res_dict, 0
        
    except Exception:
        return None, -2
        
# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ (è¶…åŠ¹ç‡åŒ–ãƒ»å®‰å…¨ç‰ˆ)
# ==========================================
def run_github_patrol():
    print("ğŸ‘® Smart Patrol Starting (JST)...")
    
    model_path = Path("boatrace_model_v3.pkl")
    config_path = Path("model_config.pkl")
    
    if not model_path.exists():
        print("Error: Model files not found.")
        return

    with open(model_path, "rb") as f: model = pickle.load(f)
    with open(config_path, "rb") as f: config = pickle.load(f)

    scraper = BoatRaceScraperV4()
    
    now_jst = datetime.now(JST)
    date_str = now_jst.strftime("%Y%m%d")
    print(f"Current Time: {now_jst.strftime('%H:%M')}")

    # 1. é–‹å‚¬å ´ã‚’å–å¾—
    courses = scraper.fetch_active_courses(date_str)
    if not courses:
        print("No races today.")
        return
    print(f"Active Courses: {len(courses)} venues")
    
    hits = []

    # 2. ä¼šå ´ã”ã¨ã«ã€Œä»Šã‚„ã‚‹ã¹ããƒ¬ãƒ¼ã‚¹ã€ã ã‘ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
    for course in courses:
        time.sleep(1) # ä¼šå ´ã”ã¨ã®ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ã¯1ç§’ã‚ã‘ã‚‹ï¼ˆå®‰å…¨ç­–ï¼‰
        
        # æ™‚åˆ»è¡¨ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ç•ªå·(R)ã‚’å–å¾—
        # ã“ã“ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã®ã¯1ãƒšãƒ¼ã‚¸ã ã‘ï¼
        target_races = scraper.get_target_races_for_course(course, date_str, now_jst)
        
        if target_races:
            print(f"Checking {course}: Race {target_races}")
            
            for rno in target_races:
                time.sleep(1) # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®ã‚¢ã‚¯ã‚»ã‚¹é–“éš”
                
                # å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ã ã‘è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦äºˆæ¸¬
                res, status = predict_single(model, config, scraper, course, rno, date_str)
                
                if status == 1 and res:
                    print(f"Found HIT! {course} {rno}R")
                    hits.append(res)
        else:
            # å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ãŒãªã„å ´åˆã¯ã‚¹ãƒ«ãƒ¼ï¼ˆãƒ­ã‚°ç¯€ç´„ã®ãŸã‚è¡¨ç¤ºã—ãªã„ã‹ã€ãƒ‰ãƒƒãƒˆã ã‘å‡ºã™ï¼‰
            print(f"{course}: No target races now.")

    # 3. é€šçŸ¥
    if hits:
        hits.sort(key=lambda x: x['ç· åˆ‡'])
        
        content = "ğŸ¯ ã‚¤ãƒ³é£›ã³ãƒœãƒ¼ãƒ€ãƒ¼è¶…ãˆç™ºå‹•\n"
        content += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        for r in hits:
            # ãƒ©ãƒ³ã‚¯åˆ¤å®š
            rank = "ğŸ”¥ã€A:å‹è² ã€‘"
            if r['ç¢ºç‡'] >= 0.65: rank = "ğŸ‘‘ã€SSS:é‰„æ¿é£›ã³ã€‘"
            elif r['ç¢ºç‡'] >= 0.60: rank = "ğŸ’ã€S:é«˜æœŸå¾…å€¤ã€‘"

            content += f"{rank}\n"
            content += f"ğŸ“ {r['å ´å']} {r['ãƒ¬ãƒ¼ã‚¹']} (ç· åˆ‡ {r['ç· åˆ‡']})\n"
            content += f"ğŸ“ˆ ç¢ºç‡: `{r['ç¢ºç‡']:.3f}` (Border: 0.570)\n"
            content += f"ğŸ•µï¸ ã‚¤ãƒ³ä¸å®‰è¦ç´ : {r['æ ¹æ‹ ']}\n"
            content += f"ğŸ”« ç‹™ã„æ’ƒã¡è»¸: `{r['ã‚¹ãƒŠã‚¤ãƒ‘ãƒ¼']}` (å±•ç¤ºæœ€é€Ÿ)\n"
            content += f"ğŸ« æ¨å¥¨: `{r['è²·ã„ç›®']}`\n"
            content += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        
        if DISCORD_WEBHOOK_URL:
            requests.post(DISCORD_WEBHOOK_URL, json={"content": content})

if __name__ == "__main__":
    run_github_patrol()
